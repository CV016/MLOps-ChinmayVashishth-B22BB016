{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Configuration and Details\n",
        "\n",
        "### Variables:\n",
        "- **Datasets**: MNIST, FashionMNIST\n",
        "- **Models**: ResNet-18, ResNet-50\n",
        "- **Batch Size**: 16\n",
        "- **Optimizers**: SGD, Adam\n",
        "- **Learning Rates**: 0.001, 0.0001\n",
        "- **Epochs**: 3, 5\n",
        "- **Pin Memory**: False, True\n",
        "\n",
        "### Constants:\n",
        "- **USE_AMP**: True (Automatic Mixed Precision enabled)\n",
        "- **Train-Val-Test Split**: 70%-10%-20%\n",
        "- **Image Size**: 64x64 (resized from 28x28)\n",
        "\n",
        "## FLOPs Calculation\n",
        "\n",
        "The `count_model_flops()` function calculates actual FLOPs (Floating Point Operations) by:\n",
        "1. **Profiling the model** with a 64×64×3 input tensor\n",
        "2. **Counting operations** in:\n",
        "   - Conv2D layers: kernel_size × channels × output_elements\n",
        "   - Linear layers: in_features × out_features  \n",
        "   - BatchNorm layers: 2 × number_of_elements\n",
        "\n",
        "This gives us the **real computational cost** for each model configuration, not just estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_split_dataset(dataset_name, batch_size, pin_memory=False):\n",
        "    transform_mnist = transforms.Compose([\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.Resize(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    \n",
        "    if dataset_name == 'MNIST':\n",
        "        full_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_mnist)\n",
        "        test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform_mnist)\n",
        "    else:\n",
        "        full_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform_mnist)\n",
        "        test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform_mnist)\n",
        "    \n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    val_size = int(0.1 * len(full_dataset))\n",
        "    remaining = len(full_dataset) - train_size - val_size\n",
        "    \n",
        "    train_dataset, val_dataset, _ = random_split(\n",
        "        full_dataset, \n",
        "        [train_size, val_size, remaining],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=pin_memory)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin_memory)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(model_name, num_classes=10):\n",
        "    if model_name == 'ResNet-18':\n",
        "        model = models.resnet18(weights=None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    elif model_name == 'ResNet-50':\n",
        "        model = models.resnet50(weights=None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device, scaler, use_amp=True):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training', leave=False)\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if use_amp and device.type == 'cuda':\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{running_loss/len(loader):.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
        "    \n",
        "    return running_loss / len(loader), 100. * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    return running_loss / len(loader), 100. * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3, use_amp=True):\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, scaler, use_amp)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        tqdm.write(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "    \n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    \n",
        "    return model, train_losses, train_accs, val_losses, val_accs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_curves(train_losses, train_accs, val_losses, val_accs, save_path):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    ax1.plot(train_losses, label='Train Loss')\n",
        "    ax1.plot(val_losses, label='Val Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    ax2.plot(train_accs, label='Train Accuracy')\n",
        "    ax2.plot(val_accs, label='Val Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_q1a = []\n",
        "epoch_values = [3, 5]\n",
        "pin_memory_values = [False, True]\n",
        "USE_AMP = True\n",
        "\n",
        "configs = [\n",
        "    {'batch_size': 16, 'optimizer': 'SGD', 'lr': 0.001},\n",
        "    {'batch_size': 16, 'optimizer': 'SGD', 'lr': 0.0001},\n",
        "    {'batch_size': 16, 'optimizer': 'Adam', 'lr': 0.001},\n",
        "    {'batch_size': 16, 'optimizer': 'Adam', 'lr': 0.0001},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_experiments = len(['MNIST', 'FashionMNIST']) * len(configs) * len(['ResNet-18', 'ResNet-50']) * len(epoch_values) * len(pin_memory_values)\n",
        "exp_counter = 0\n",
        "\n",
        "for dataset_name in ['MNIST', 'FashionMNIST']:\n",
        "    for config in configs:\n",
        "        batch_size = config['batch_size']\n",
        "        opt_name = config['optimizer']\n",
        "        lr = config['lr']\n",
        "        \n",
        "        for pin_mem in pin_memory_values:\n",
        "            train_loader, val_loader, test_loader = load_and_split_dataset(dataset_name, batch_size, pin_mem)\n",
        "            \n",
        "            for model_name in ['ResNet-18', 'ResNet-50']:\n",
        "                for epochs in epoch_values:\n",
        "                    exp_counter += 1\n",
        "                    exp_name = f\"{dataset_name}_{model_name}_bs{batch_size}_{opt_name}_lr{lr}_ep{epochs}_pm{pin_mem}\"\n",
        "                    print(f\"\\n{'='*80}\")\n",
        "                    print(f\"Experiment {exp_counter}/{total_experiments}: {exp_name}\")\n",
        "                    print(f\"{'='*80}\")\n",
        "                    \n",
        "                    model = get_model(model_name, num_classes=10).to(device)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    \n",
        "                    if opt_name == 'SGD':\n",
        "                        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "                    else:\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    \n",
        "                    model, train_losses, train_accs, val_losses, val_accs = train_model(\n",
        "                        model, train_loader, val_loader, optimizer, criterion, device, epochs, USE_AMP\n",
        "                    )\n",
        "                    \n",
        "                    _, test_acc = validate(model, test_loader, criterion, device)\n",
        "                    \n",
        "                    torch.save(model.state_dict(), f'models/{exp_name}.pth')\n",
        "                    plot_training_curves(train_losses, train_accs, val_losses, val_accs, \n",
        "                                       f'results/{exp_name}.png')\n",
        "                    \n",
        "                    print(f\"✓ Completed - Test Accuracy: {test_acc:.2f}%\")\n",
        "                    \n",
        "                    results_q1a.append({\n",
        "                        'Dataset': dataset_name,\n",
        "                        'Batch Size': batch_size,\n",
        "                        'Optimizer': opt_name,\n",
        "                        'Learning Rate': lr,\n",
        "                        'Epochs': epochs,\n",
        "                        'Pin Memory': pin_mem,\n",
        "                        'USE_AMP': USE_AMP,\n",
        "                        'Model': model_name,\n",
        "                        'Test Accuracy': f\"{test_acc:.2f}\"\n",
        "                    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_q1a = pd.DataFrame(results_q1a)\n",
        "df_q1a.to_csv('results/q1a_results.csv', index=False)\n",
        "df_q1a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset_for_svm(dataset_name):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "    \n",
        "    if dataset_name == 'MNIST':\n",
        "        train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "    else:\n",
        "        train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "        test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "    \n",
        "    X_train = train_dataset.data.numpy().reshape(len(train_dataset), -1) / 255.0\n",
        "    y_train = train_dataset.targets.numpy()\n",
        "    X_test = test_dataset.data.numpy().reshape(len(test_dataset), -1) / 255.0\n",
        "    y_test = test_dataset.targets.numpy()\n",
        "    \n",
        "    sample_size = min(10000, len(X_train))\n",
        "    indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_q1b = []\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Q1(b) - SVM Training\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for dataset_name in ['MNIST', 'FashionMNIST']:\n",
        "    X_train, y_train, X_test, y_test = load_dataset_for_svm(dataset_name)\n",
        "    \n",
        "    for kernel in ['poly', 'rbf']:\n",
        "        print(f\"\\nTraining SVM on {dataset_name} with {kernel} kernel...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        svm = SVC(kernel=kernel, gamma='scale', max_iter=1000)\n",
        "        svm.fit(X_train, y_train)\n",
        "        \n",
        "        train_time = (time.time() - start_time) * 1000\n",
        "        \n",
        "        y_pred = svm.predict(X_test)\n",
        "        test_acc = accuracy_score(y_test, y_pred) * 100\n",
        "        \n",
        "        print(f\"✓ Completed - Test Accuracy: {test_acc:.2f}%, Training Time: {train_time:.2f}ms\")\n",
        "        \n",
        "        results_q1b.append({\n",
        "            'Dataset': dataset_name,\n",
        "            'Kernel': kernel,\n",
        "            'Test Accuracy': f\"{test_acc:.2f}\",\n",
        "            'Training Time (ms)': f\"{train_time:.2f}\"\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_q1b = pd.DataFrame(results_q1b)\n",
        "df_q1b.to_csv('results/q1b_results.csv', index=False)\n",
        "df_q1b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_model_flops(model, input_size=64, device='cpu'):\n",
        "    model_copy = model.to('cpu')\n",
        "    model_copy.eval()\n",
        "    \n",
        "    input_tensor = torch.randn(1, 3, input_size, input_size)\n",
        "    \n",
        "    total_flops = 0\n",
        "    \n",
        "    def count_conv2d(m, x, y):\n",
        "        nonlocal total_flops\n",
        "        cin = m.in_channels\n",
        "        kernel_ops = m.kernel_size[0] * m.kernel_size[1] * (cin // m.groups)\n",
        "        output_elements = y.numel()\n",
        "        total_flops += kernel_ops * output_elements\n",
        "    \n",
        "    def count_linear(m, x, y):\n",
        "        nonlocal total_flops\n",
        "        total_flops += m.in_features * m.out_features\n",
        "    \n",
        "    def count_bn(m, x, y):\n",
        "        nonlocal total_flops\n",
        "        total_flops += 2 * x[0].numel()\n",
        "    \n",
        "    hooks = []\n",
        "    for name, module in model_copy.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            hooks.append(module.register_forward_hook(count_conv2d))\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            hooks.append(module.register_forward_hook(count_linear))\n",
        "        elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "            hooks.append(module.register_forward_hook(count_bn))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model_copy(input_tensor)\n",
        "    \n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "    \n",
        "    model.to(device)\n",
        "    return total_flops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_resnet18 = get_model('ResNet-18', num_classes=10)\n",
        "sample_resnet50 = get_model('ResNet-50', num_classes=10)\n",
        "\n",
        "flops_r18 = count_model_flops(sample_resnet18, input_size=64)\n",
        "flops_r50 = count_model_flops(sample_resnet50, input_size=64)\n",
        "\n",
        "print(f\"Actual FLOPs for 64x64 input:\")\n",
        "print(f\"ResNet-18: {flops_r18:,} FLOPs ({flops_r18:.2e})\")\n",
        "print(f\"ResNet-50: {flops_r50:,} FLOPs ({flops_r50:.2e})\")\n",
        "\n",
        "del sample_resnet18, sample_resnet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_q2 = []\n",
        "dataset_name = 'FashionMNIST'\n",
        "batch_size = 16\n",
        "epoch_values_q2 = [3, 5]\n",
        "pin_memory_values_q2 = [False, True]\n",
        "USE_AMP_Q2 = True\n",
        "\n",
        "configs_q2 = [\n",
        "    {'optimizer': 'SGD', 'lr': 0.001},\n",
        "    {'optimizer': 'Adam', 'lr': 0.001},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Q2 - CPU vs GPU Performance\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "for compute in ['CPU', 'GPU']:\n",
        "    device_q2 = torch.device('cpu' if compute == 'CPU' else 'cuda')\n",
        "    \n",
        "    if compute == 'GPU' and not torch.cuda.is_available():\n",
        "        print(f\"\\n⚠ GPU not available, skipping GPU experiments\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n--- Running on {compute} ---\")\n",
        "    \n",
        "    for pin_mem in pin_memory_values_q2:\n",
        "        train_loader, val_loader, test_loader = load_and_split_dataset(dataset_name, batch_size, pin_mem)\n",
        "        \n",
        "        for config in configs_q2:\n",
        "            opt_name = config['optimizer']\n",
        "            lr = config['lr']\n",
        "            \n",
        "            for model_name in ['ResNet-18', 'ResNet-50']:\n",
        "                for epochs in epoch_values_q2:\n",
        "                    exp_name = f\"{compute}_{model_name}_{opt_name}_lr{lr}_ep{epochs}_pm{pin_mem}\"\n",
        "                    print(f\"\\n{exp_name}\")\n",
        "                    \n",
        "                    model = get_model(model_name, num_classes=10).to(device_q2)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    \n",
        "                    if opt_name == 'SGD':\n",
        "                        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "                    else:\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    \n",
        "                    start_time = time.time()\n",
        "                    model, _, _, _, _ = train_model(\n",
        "                        model, train_loader, val_loader, optimizer, criterion, device_q2, epochs, USE_AMP_Q2\n",
        "                    )\n",
        "                    train_time = (time.time() - start_time) * 1000\n",
        "                    \n",
        "                    _, test_acc = validate(model, test_loader, criterion, device_q2)\n",
        "                    \n",
        "                    flops = count_model_flops(model, input_size=64, device=device_q2)\n",
        "                    \n",
        "                    print(f\"✓ Test Accuracy: {test_acc:.2f}%, Training Time: {train_time:.2f}ms, FLOPs: {flops:.2e}\")\n",
        "                    \n",
        "                    results_q2.append({\n",
        "                        'Compute': compute,\n",
        "                        'Batch Size': batch_size,\n",
        "                        'Optimizer': opt_name,\n",
        "                        'Learning Rate': lr,\n",
        "                        'Epochs': epochs,\n",
        "                        'Pin Memory': pin_mem,\n",
        "                        'USE_AMP': USE_AMP_Q2,\n",
        "                        'Model': model_name,\n",
        "                        'Test Accuracy': f\"{test_acc:.2f}\",\n",
        "                        'Training Time (ms)': f\"{train_time:.2f}\",\n",
        "                        'FLOPs': f\"{flops:.2e}\"\n",
        "                    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_q2 = pd.DataFrame(results_q2)\n",
        "df_q2.to_csv('results/q2_results.csv', index=False)\n",
        "df_q2\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
