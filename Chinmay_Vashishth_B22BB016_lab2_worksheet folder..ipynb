{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0yRnIjGXvZZt"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D8-04VnUwqks",
        "outputId": "b9228eb8-4fe0-415e-e4ba-2b8229ad8a1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 33e900f63f144527de34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Invalid API key: API key must have 40+ characters, has 20.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 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\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb22bb016\u001b[0m (\u001b[33mb22bb016-prom-iit-rajasthan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260131_203514-66w4h8y7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10/runs/66w4h8y7' target=\"_blank\">CNN_Run_Colab</a></strong> to <a href='https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10' target=\"_blank\">https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10/runs/66w4h8y7' target=\"_blank\">https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10/runs/66w4h8y7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total FLOPs: 76.55 Million\n",
            "Epoch [1/25] Loss: 1.5830\n",
            "Epoch [2/25] Loss: 1.2803\n",
            "Epoch [3/25] Loss: 1.1582\n",
            "Epoch [4/25] Loss: 1.0808\n",
            "Epoch [5/25] Loss: 1.0143\n",
            "Epoch [6/25] Loss: 0.9694\n",
            "Epoch [7/25] Loss: 0.9256\n",
            "Epoch [8/25] Loss: 0.8941\n",
            "Epoch [9/25] Loss: 0.8629\n",
            "Epoch [10/25] Loss: 0.8382\n",
            "Epoch [11/25] Loss: 0.8092\n",
            "Epoch [12/25] Loss: 0.7853\n",
            "Epoch [13/25] Loss: 0.7629\n",
            "Epoch [14/25] Loss: 0.7451\n",
            "Epoch [15/25] Loss: 0.7256\n",
            "Epoch [16/25] Loss: 0.7058\n",
            "Epoch [17/25] Loss: 0.6913\n",
            "Epoch [18/25] Loss: 0.6761\n",
            "Epoch [19/25] Loss: 0.6616\n",
            "Epoch [20/25] Loss: 0.6478\n",
            "Epoch [21/25] Loss: 0.6323\n",
            "Epoch [22/25] Loss: 0.6277\n",
            "Epoch [23/25] Loss: 0.6108\n",
            "Epoch [24/25] Loss: 0.6049\n",
            "Epoch [25/25] Loss: 0.5934\n",
            "Training Complete. Check WandB dashboard for charts.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Total FLOPs (M)</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>loss</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Total FLOPs (M)</td><td>76.54734</td></tr><tr><td>epoch</td><td>25</td></tr><tr><td>loss</td><td>0.5934</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CNN_Run_Colab</strong> at: <a href='https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10/runs/66w4h8y7' target=\"_blank\">https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10/runs/66w4h8y7</a><br> View project at: <a href='https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10' target=\"_blank\">https://wandb.ai/b22bb016-prom-iit-rajasthan/colab-cifar10</a><br>Synced 5 W&B file(s), 25 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260131_203514-66w4h8y7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "class CustomCIFAR10(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None):\n",
        "        # Download data to Colab's local storage\n",
        "        self.cifar_raw = torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
        "        self.data = self.cifar_raw.data\n",
        "        self.targets = self.cifar_raw.targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        target = self.targets[idx]\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Block 1\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Block 2\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Classifier\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def count_flops(model, input_size=(1, 3, 32, 32)):\n",
        "    flops = []\n",
        "\n",
        "    def conv_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
        "        bias_ops = 1 if self.bias is not None else 0\n",
        "        params = kernel_ops + bias_ops\n",
        "        flops.append(batch_size * params * output_channels * output_height * output_width)\n",
        "\n",
        "    def linear_hook(self, input, output):\n",
        "        batch_size = input[0].size(0)\n",
        "        weight_ops = self.weight.nelement()\n",
        "        bias_ops = self.bias.nelement() if self.bias is not None else 0\n",
        "        flops.append(batch_size * (weight_ops + bias_ops))\n",
        "\n",
        "    hooks = []\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            hooks.append(layer.register_forward_hook(conv_hook))\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            hooks.append(layer.register_forward_hook(linear_hook))\n",
        "\n",
        "    # Dummy pass\n",
        "    dummy_input = torch.randn(input_size).to(next(model.parameters()).device)\n",
        "    model(dummy_input)\n",
        "\n",
        "    for hook in hooks: hook.remove()\n",
        "    return sum(flops)\n",
        "\n",
        "def plot_grad_flow(named_parameters):\n",
        "    ave_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n) and (p.grad is not None):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean().cpu().item())\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def train():\n",
        "    # Initialize WandB\n",
        "    run = wandb.init(project=\"colab-cifar10\", name=\"CNN_Run_Colab\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training on: {device}\")\n",
        "\n",
        "    # Hyperparams\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 25\n",
        "    LR = 0.001\n",
        "\n",
        "    # Data\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    train_dataset = CustomCIFAR10(root='./data', train=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    model = SimpleCNN().to(device)\n",
        "\n",
        "    # Calculate FLOPs\n",
        "    total_flops = count_flops(model)\n",
        "    print(f\"Total FLOPs: {total_flops / 1e6:.2f} Million\")\n",
        "    wandb.log({\"Total FLOPs (M)\": total_flops / 1e6})\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    # Track old weights for visualization\n",
        "    old_weights = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            old_weights[name] = param.clone().detach()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Log Gradient Flow (Last batch of epoch)\n",
        "            if i == len(train_loader) - 1:\n",
        "                grad_fig = plot_grad_flow(model.named_parameters())\n",
        "                wandb.log({\"Gradient Flow\": wandb.Image(grad_fig)}, commit=False)\n",
        "                plt.close(grad_fig)\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Log Weight Updates\n",
        "        update_magnitudes = {}\n",
        "        with torch.no_grad():\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    new_weight = param\n",
        "                    update = (new_weight - old_weights[name]).abs().mean().item()\n",
        "                    update_magnitudes[name] = update\n",
        "                    old_weights[name] = new_weight.clone().detach() # Update reference\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss, \"weight_updates\": update_magnitudes})\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Training Complete. Check WandB dashboard for charts.\")\n",
        "    wandb.finish()\n",
        "\n",
        "# Run the training\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
