# Evaluation package: metrics, evaluation, reporting, and plots.

from eval.metrics import compute_accuracy_metrics
from eval.evaluate import evaluate_trainer, get_predictions
from eval.report import print_classification_report, print_sample_predictions
from eval.plots import plot_confusion_heatmap, plot_misclassification_heatmap
from eval.save_results import run_evaluation_and_save

__all__ = [
    "compute_accuracy_metrics",
    "evaluate_trainer",
    "get_predictions",
    "print_classification_report",
    "print_sample_predictions",
    "plot_confusion_heatmap",
    "plot_misclassification_heatmap",
    "run_evaluation_and_save",
]
